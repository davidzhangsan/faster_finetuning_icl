{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fcfaf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#####################################################\n",
    "# In this notebook, we train a 6-layer linear transformer with\n",
    "# - context-length 64\n",
    "# - covariate dimension 16, standard Gaussian distribution (same setup as GPT-2)\n",
    "# - \n",
    "# We plot\n",
    "# - test loss against number of iterations\n",
    "# - imshow of each parameter matrix at end of training\n",
    "# - distance-to-identity of each parameter matrix (this is shown in theory)\n",
    "#####################################################\n",
    "\n",
    "#use cuda if available, else use cpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#torch.cuda.set_device(1)\n",
    "# import the model and some useful functions\n",
    "from linear_transformer import Transformer_F, attention, generate_data, in_context_loss\n",
    "\n",
    "# set up some print options\n",
    "np.set_printoptions(precision = 2, suppress = True)\n",
    "torch.set_printoptions(precision=2)\n",
    "\n",
    "#begin logging\n",
    "log_dir = 'log' \n",
    "#exp_dir = 'simple_demonstration' \n",
    "cur_dir = log_dir #os.path.join(log_dir, exp_dir)\n",
    "os.makedirs(cur_dir, exist_ok=True)\n",
    "#f = open(cur_dir + '/train.log', \"a\", 1)\n",
    "#sys.stdout = f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9700bf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up problem parameters\n",
    "\n",
    "lr = 0.001\n",
    "clip_r = 1000\n",
    "alg = 'adam' # 'adam' or 'sgd'\n",
    "mode = 'normal'\n",
    "\n",
    "n_layer = 6 # number of layers of transformer\n",
    "N = 64     # context length\n",
    "d = 12        # dimension\n",
    "\n",
    "\n",
    "n_head = 2 # 2-headed attention\n",
    "B = 64  # 1000 minibatch size\n",
    "var = 0.0001  # initializations scale of transformer parameter\n",
    "shape_k = 0.1  # shape_k: parameter for Gamma distributed covariates\n",
    "max_iters = 5000  # Number of Iterations to run\n",
    "hist_stride = 1  # stride for saved model paramters in `train.ipynb'\n",
    "stride = 100\n",
    "\n",
    "# a convenience function for taking a step and clipping\n",
    "def clip_and_step(allparam, optimizer, clip_r = None):\n",
    "    norm_p=None\n",
    "    grad_all = allparam.grad\n",
    "    if clip_r is not None:\n",
    "        norm_p = grad_all.norm().item()\n",
    "        if norm_p > clip_r:\n",
    "            grad_all.mul_(clip_r/norm_p)\n",
    "    optimizer.step()\n",
    "    return norm_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69d0ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adam training\n",
      "iter 0 | Loss: 8.107519149780273  time: 9.630950927734375  gradnorm: 39.835365295410156\n",
      "iter 1 | Loss: 10.212871551513672  time: 0.9338619709014893  gradnorm: 58.21845245361328\n",
      "iter 2 | Loss: 10.617920875549316  time: 0.027786970138549805  gradnorm: 42.2610969543457\n",
      "iter 3 | Loss: 12.267753601074219  time: 0.0438532829284668  gradnorm: 47.695220947265625\n",
      "iter 4 | Loss: 13.178829193115234  time: 0.042882680892944336  gradnorm: 54.59962844848633\n",
      "iter 100 | Loss: 2.4177308082580566  time: 0.02859187126159668  gradnorm: 9.860877990722656\n",
      "iter 200 | Loss: 0.39449894428253174  time: 0.11449074745178223  gradnorm: 2.2263448238372803\n",
      "iter 300 | Loss: 0.22041065990924835  time: 0.3040120601654053  gradnorm: 4.404223918914795\n"
     ]
    }
   ],
   "source": [
    "filename_format = '/linearTF_exp_{}_{}_{}_{}.pth'\n",
    "filename = filename_format.format(n_layer, N, d, alg)\n",
    "filename = (cur_dir + filename)\n",
    "hist_dict = {}\n",
    "\n",
    "\n",
    "seeds = [0] # use one seed \n",
    "keys = [(s,) for s in seeds]\n",
    "for key in keys:\n",
    "    sd = key[0]\n",
    "    \n",
    "    prob_seed = sd\n",
    "    opt_seed = sd\n",
    "    \n",
    "    hist_dict[key] = []\n",
    "    \n",
    "    #set seed and initialize model\n",
    "    torch.manual_seed(opt_seed)\n",
    "    model = Transformer_F(n_layer, n_head, d, var)\n",
    "    model.to(device)\n",
    "    #initialize algorithm. Important: set beta = 0.9 for adam, 0.999 is very slow\n",
    "    if alg == 'sgd':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=0)\n",
    "        print(\"sgd training\")\n",
    "    elif alg == 'adam':\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=lr, betas=(0.9, 0.9), weight_decay=0)\n",
    "        print(\"adam training\")\n",
    "    else: assert False\n",
    "    \n",
    "    #set seed and initialize initial training batch\n",
    "    np.random.seed(prob_seed)\n",
    "    torch.manual_seed(prob_seed)\n",
    "    \n",
    "    for t in range(max_iters):\n",
    "        start = time.time()\n",
    "        # save model parameters\n",
    "        if t%hist_stride ==0:\n",
    "            hist_dict[key].append(model.allparam.clone().detach())\n",
    "        #  generate a new batch of training set\n",
    "        Z, y = generate_data(mode,N,d,B,shape_k)\n",
    "        Z = Z.to(device)\n",
    "        y = y.to(device)\n",
    "        loss = in_context_loss(model, Z, y)\n",
    "        \n",
    "        # compute gradient, take step\n",
    "        loss.backward()\n",
    "        norms = clip_and_step(model.allparam, optimizer, clip_r=clip_r)\n",
    "        optimizer.zero_grad()\n",
    "        end=time.time()\n",
    "        if t%100 ==0 or t<5:\n",
    "            print('iter {} | Loss: {}  time: {}  gradnorm: {}'.format(t,loss.item(), end-start, norms))\n",
    "    #save to \n",
    "torch.save({'hist_dict':hist_dict}, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83154b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "# compute test loss\n",
    "####################################\n",
    "hist_dict = torch.load(filename)['hist_dict']\n",
    "loss_dict = {}\n",
    "for key in hist_dict:\n",
    "    sd = key[0]\n",
    "    \n",
    "    loss_dict[key] = torch.zeros(max_iters // stride)\n",
    "    \n",
    "    np.random.seed(99)\n",
    "    torch.manual_seed(99)\n",
    "    Z, y = generate_data(mode, N, d, B, shape_k)\n",
    "    Z = Z.to(device)\n",
    "    y = y.to(device)\n",
    "    model = Transformer_F(n_layer, n_head, d, var).to(device)\n",
    "    print(sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "    for t in range(0,max_iters,stride):\n",
    "        with torch.no_grad():\n",
    "            model.allparam.copy_(hist_dict[key][t])\n",
    "        loss_dict[key][t // stride] = in_context_loss(model, Z, y).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c41abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "# plot the test loss with error bars\n",
    "####################################\n",
    "\n",
    "fig_dir = 'figures' \n",
    "os.makedirs(fig_dir, exist_ok=True)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1,figsize = (7, 6))\n",
    "\n",
    "losses = torch.zeros(len(seeds), max_iters//stride)\n",
    "keys = loss_dict.keys()\n",
    "for idx, key in enumerate(keys):\n",
    "    losses[idx,:] = loss_dict[key]\n",
    "losses_mean = torch.mean(losses, axis=0)\n",
    "losses_std = torch.std(losses, axis=0)\n",
    "ax.plot(range(0,max_iters,stride), losses_mean, color = 'red', lw = 3)#, label='Adam')\n",
    "ax.fill_between(range(0,max_iters,stride), losses_mean-losses_std, losses_mean+losses_std, color = 'red', alpha = 0.2)\n",
    "ax.set_xlabel('Iteration',fontsize=40)\n",
    "ax.set_ylabel('ICL Test Loss',fontsize=40)\n",
    "ax.tick_params(axis='both', which='major', labelsize=30, width = 3, length = 10)\n",
    "ax.tick_params(axis='both', which='minor', labelsize=20, width = 3, length = 5)\n",
    "#ax.legend(fontsize=30)\n",
    "ax.set_yscale('log')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(fig_dir + '/simple_demonstration_loss_plot + {filename}.pdf', dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88adc8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "# Display B and A Matrices in a Combined Grid Layout\n",
    "####################################\n",
    "\n",
    "key = (0,)\n",
    "\n",
    "# Setup a grid with 2 rows: Top for B matrices, Bottom for A matrices\n",
    "fig, axes = plt.subplots(2, n_layer, figsize=(4 * n_layer, 12), constrained_layout=True)\n",
    "\n",
    "# Plot B matrices (Top row)\n",
    "for l in range(n_layer):\n",
    "    ax = axes[0, l]\n",
    "    matrix_B = hist_dict[key][max_iters-1][l, 0, 0, :, :]  # Access B matrix\n",
    "    im = ax.imshow(matrix_B.cpu(), cmap='coolwarm', vmin=-0.5, vmax=0.5)\n",
    "    \n",
    "    # Annotate significant values\n",
    "    for i in range(matrix_B.shape[0]):\n",
    "        for j in range(matrix_B.shape[1]):\n",
    "            value = matrix_B[i, j].item()\n",
    "            if abs(value) > 0.05:\n",
    "                ax.text(j, i, f\"{value:.2f}\", ha='center', va='center', color='black', fontsize=8)\n",
    "    ax.set_title(f\"$B_{l}$\", fontsize=16)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "# Plot A matrices (Bottom row)\n",
    "for l in range(n_layer):\n",
    "    ax = axes[1, l]\n",
    "    matrix_A = hist_dict[key][max_iters-1][l, 0, 1, :, :]  # Access A matrix\n",
    "    im = ax.imshow(matrix_A.cpu(), cmap='coolwarm', vmin=-0.5, vmax=0.5)\n",
    "    \n",
    "    # Annotate significant values\n",
    "    for i in range(matrix_A.shape[0]):\n",
    "        for j in range(matrix_A.shape[1]):\n",
    "            value = matrix_A[i, j].item()\n",
    "            if abs(value) > 0.05:\n",
    "                ax.text(j, i, f\"{value:.2f}\", ha='center', va='center', color='black', fontsize=8)\n",
    "    ax.set_title(f\"$A_{l}$\", fontsize=16)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "# Add a single colorbar for the entire figure\n",
    "fig.colorbar(im, ax=axes, location='right', fraction=0.02, pad=0.02)\n",
    "\n",
    "# Set the overall title and save the figure\n",
    "fig.suptitle(\"All $B$ and $A$ Matrices Across Layers\", fontsize=20)\n",
    "plt.savefig(fig_dir + '/all_B_A_layers_combined.pdf', dpi=600)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a10c824",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "# plot the distance-to-identity of each matrix with time\n",
    "########################################################\n",
    "\n",
    "# function for computing distance to identity\n",
    "def compute_dist_identity(M):\n",
    "    scale = torch.sum(torch.diagonal(M))/M.shape[0]\n",
    "    ideal_identity = scale* torch.eye(M.shape[0]).to(device)\n",
    "    difference = M - ideal_identity\n",
    "    err = (torch.norm(difference,p='fro')/torch.norm(M,p='fro')).item()\n",
    "    return err\n",
    "\n",
    "########################################\n",
    "# compute distances (assume n_head = 1)\n",
    "########################################\n",
    "dist_dict = {}\n",
    "            \n",
    "for key in hist_dict:\n",
    "    (sd,) = key\n",
    "    dist_dict[key] = torch.zeros(n_layer, 2, max_iters//stride)\n",
    "\n",
    "    for t in range(0,max_iters,stride):\n",
    "        with torch.no_grad():\n",
    "            allparam = hist_dict[key][t]\n",
    "        for i in range(n_layer):\n",
    "            for j in range(2):\n",
    "                dist_dict[key][i,j,t//stride] = compute_dist_identity(allparam[i,0,j,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9515b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "# Plot distances with corrected subplot indexing\n",
    "####################################\n",
    "\n",
    "fig_dir = 'figures'\n",
    "os.makedirs(fig_dir, exist_ok=True)\n",
    "\n",
    "# Dynamically set grid size: n_layer rows, 2 columns (B and A matrices)\n",
    "fig, axs = plt.subplots(n_layer, 2, figsize=(12, 4 * n_layer), constrained_layout=True)\n",
    "\n",
    "labels = ['B', 'A']  # For the two types of matrices\n",
    "colors = ['red', 'blue']  # Colors for distinction\n",
    "\n",
    "for l in range(n_layer):  # Loop through layers\n",
    "    for pq in range(2):  # 0 for B, 1 for A\n",
    "        ax = axs[l, pq]  # Access subplot dynamically\n",
    "        dist_p = torch.zeros(len(seeds), max_iters // stride)\n",
    "        \n",
    "        # Gather distances for the current matrix\n",
    "        for idx, sd in enumerate(seeds):\n",
    "            dist_p[idx, :] = dist_dict[(sd,)][l, pq, :]\n",
    "        \n",
    "        dist_mean = torch.mean(dist_p, axis=0)\n",
    "        dist_std = torch.std(dist_p, axis=0)\n",
    "\n",
    "        # Plot distance\n",
    "        ax.plot(range(0, max_iters, stride), dist_mean, color=colors[pq], lw=3, label=f\"{labels[pq]}_{l}\")\n",
    "        ax.fill_between(range(0, max_iters, stride), dist_mean - dist_std, dist_mean + dist_std, \n",
    "                        color=colors[pq], alpha=0.2)\n",
    "        ax.set_yscale('log')\n",
    "        ax.legend(fontsize=10)\n",
    "        ax.set_title(f\"Distance-to-Identity: {labels[pq]}_{l}\", fontsize=14)\n",
    "        ax.set_xlabel(\"Iteration\", fontsize=12)\n",
    "        ax.set_ylabel(\"Distance\", fontsize=12)\n",
    "        ax.tick_params(axis='both', labelsize=10)\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig(fig_dir + '/distance_to_identity_fixed.pdf', dpi=600)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ca8fff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers_icl_opt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
