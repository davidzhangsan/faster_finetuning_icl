from dataclasses import dataclass
from typing import Optional

@dataclass
class TransformerConfig:
    num_heads: int = 2
    widening_factor: int = 4
    num_layers: int = 3
    key_size: int = 5
    embedding_size: int = 64
    output_size: int = 1
    in_context_length: int = 17
    in_context_length_test: int = 17
    test_points: int = 1
    dropout_rate: float = 0.0
    only_attention: bool = True
    use_layer_norm: bool = True
    use_pe: bool = True
    pe_size: int = 6
    concat_pe: bool = False
    output_mapping: bool = False
    input_mapping: bool = False
    use_bias_p: bool = True
    zero_embeddings: bool = False
    deq: bool = True
    init_scale: float = 0.02
    use_softmax: bool = False
    use_non_lin_mix: bool = False
    first_layer_sm: bool = False
    y_update: bool = False
    input_mlp: bool = False
    input_mlp_out_dim: int = 0
    gd_mlp_config: bool = False
    sum_norm: bool = False
    dampening: float = 1.0
    clip: float = 0.0
    ana_copy: bool = False
    flip: bool = False
    vocab_size: int = 0
    vocab_token_dim: int = 0
    vocab_init: float = 0.01
    return_logits: bool = False
    include_query: bool = False
    name: Optional[str] = None