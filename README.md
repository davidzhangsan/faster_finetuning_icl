# faster_finetuning_icl

This is our attempt to replicate & verify GPT-2 and two linear transformer setups for in-context linear regression.

- davidzlx: contains code for GPT-2 full scale and Linear Transformer (Ahn).
- tobby: contains code for Linear Self Attention (von Oswald)
- yingyx: contains code for LoRA on GPT-2.
